{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Here Wav2lip has been used for lip sync a video wit desired audio\n",
        "2. As generated video had very low qualty and face became very blurry, used restoration GAN named GFPGAN\n",
        "\n",
        "Reference\n",
        "- https://github.com/zabique/Wav2Lip\n",
        "- https://github.com/TencentARC/GFPGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT9AQwdf8sJK"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ5taGmPcWV-"
      },
      "source": [
        "# **Get the code and models for**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qgo-oaI3JU2u",
        "outputId": "b6beb03f-084d-4b09-bc8a-982a0bd05df1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title <h1>Install Wav2Lip</h1>\n",
        "#@markdown * Install dependencies\n",
        "#@markdown * Download models\n",
        "!rm -rf /content/sample_data\n",
        "!mkdir /content/sample_data\n",
        "\n",
        "!git clone https://github.com/zabique/Wav2Lip\n",
        "\n",
        "#download the pretrained model\n",
        "!wget 'https://iiitaphyd-my.sharepoint.com/personal/radrabha_m_research_iiit_ac_in/_layouts/15/download.aspx?share=EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA' -O '/content/Wav2Lip/checkpoints/wav2lip_gan.pth'\n",
        "!wget 'https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/Eb3LEzbfuKlJiR600lQWRxgBIY27JZg80f7V9jtMfbNDaQ?e=TBFBVW' -O '/content/Wav2Lip/checkpoints/wav2lip.pth'\n",
        "!pip install https://raw.githubusercontent.com/AwaleSajil/ghc/master/ghc-1.0-py3-none-any.whl\n",
        "\n",
        "# !pip uninstall tensorflow tensorflow-gpu\n",
        "!cd Wav2Lip && pip install -r requirements.txt\n",
        "\n",
        "#download pretrained model for face detection\n",
        "!wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O \"/content/Wav2Lip/face_detection/detection/sfd/s3fd.pth\"\n",
        "\n",
        "!pip install -q youtube-dl\n",
        "!pip install ffmpeg-python\n",
        "from IPython.display import clear_output \n",
        "clear_output()\n",
        "print(\"\\nDone\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzokJMO19IyY"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgjaWJFs8B38"
      },
      "source": [
        "# **Quick guide**\n",
        "1. Create video file input_video.mp4\n",
        "2. Create audio file input_audio.wav\n",
        "3. Both files have to be exact same length\n",
        "4. Target face in the input_video.mp4, must be in ALL videoframes (So no black or blurry frames etc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdIQfY2Kswcb"
      },
      "source": [
        "# **Now lets try!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "vsphzJawLF-f",
        "outputId": "137fd255-49ae-4375-bcbf-8d646938d4e4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title 1.Upload input_video.mp4 & input_audio.wav files\n",
        "%cd sample_data/\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOkGvarvOeFP",
        "outputId": "8bf0be26-390e-4f16-be9b-4eff62518bf8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Method 3\n",
        "# import scipy\n",
        "# from scipy.io import wavfile\n",
        "  \n",
        "# # function to convert the information into \n",
        "# # some readable format\n",
        "# def output_duration(length):\n",
        "#     hours = length // 3600  # calculate in hours\n",
        "#     length %= 3600\n",
        "#     mins = length // 60  # calculate in minutes\n",
        "#     length %= 60\n",
        "#     seconds = length  # calculate in seconds\n",
        "  \n",
        "#     return hours, mins, seconds\n",
        "  \n",
        "# # sample_rate holds the sample rate of the wav file\n",
        "# # in (sample/sec) format\n",
        "# # data is the numpy array that consists\n",
        "# # of actual data read from the wav file\n",
        "# sample_rate, data = wavfile.read('/content/sample_data/input_audio.wav')\n",
        "  \n",
        "# len_data = len(data)  # holds length of the numpy array\n",
        "  \n",
        "# t = len_data / sample_rate  # returns duration but in floats\n",
        "  \n",
        "# hours, mins, seconds = output_duration(int(t))\n",
        "# print('Total Duration: {}:{}:{}'.format(hours, mins, seconds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPAWEi4GOtOm",
        "outputId": "f3e87a3d-4a4c-4934-c198-547a3bad7102",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# # import module\n",
        "# import cv2\n",
        "# import datetime\n",
        " \n",
        "# # create video capture object\n",
        "# data = cv2.VideoCapture('/content/sample_data/input_video.mp4')\n",
        " \n",
        "# # count the number of frames\n",
        "# frames = data.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "# fps = data.get(cv2.CAP_PROP_FPS)\n",
        "# width  = data.get(cv2.CAP_PROP_FRAME_WIDTH)   # float `width`\n",
        "# height = data.get(cv2.CAP_PROP_FRAME_HEIGHT)  \n",
        "# # if data.isOpened(): \n",
        "# #     # get vcap property \n",
        "# #     width  = data.get(cv2.cv.CV_CAP_PROP_FRAME_WIDTH)   # float `width`\n",
        "# #     height = data.get(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT)  # float `height`\n",
        "# #     # or\n",
        "# #     width  = data.get(3)  # float `width`\n",
        "# #     height = data.get(4)  # float `height`\n",
        "\n",
        "# #     # it gives me 0.0 :/\n",
        "# #     fps = vcap.get(cv2.cv.CV_CAP_PROP_FPS)\n",
        "\n",
        "# # calculate duration of the video\n",
        "# seconds = round(frames / fps)\n",
        "# video_time = datetime.timedelta(seconds=seconds)\n",
        "# print(f\"width: {width}\")\n",
        "# print(f\"height: {height}\")\n",
        "\n",
        "# print(f\"duration in seconds: {seconds}\")\n",
        "# print(f\"video time: {video_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR5utmDMcSZY",
        "outputId": "4f727731-0978-4c08-a458-e12c45df803c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title 2.Create Wav2Lip video (using wav2lip_gan.pth) GAN\n",
        "!cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"/content/sample_data/input_video.mp4\" --audio \"/content/sample_data/input_audio.wav\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "WxbzXZvLliiA",
        "outputId": "4e173d36-904a-4a10-866f-975190f920ca",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title 3.Play result video -  50% scaling\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('/content/Wav2Lip/results/result_voice.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"50%\" height=\"50%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1kt-krsEbz5j",
        "outputId": "3cb00454-e3ac-4b12-ce5a-0212cdef9c8b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title 4.Download Result.mp4 to your computer\n",
        "from google.colab import files\n",
        "files.download('/content/Wav2Lip/results/result_voice.mp4') \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT8njpBCJ7gD",
        "outputId": "84523b14-c900-47d2-e0d3-bafafbf06003",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title 5. Delete old result files\n",
        "%rm /content/sample_data/*\n",
        "%rm /content/Wav2Lip/results/*\n",
        "from IPython.display import clear_output \n",
        "clear_output()\n",
        "print(\"\\nDone! now press X\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgMkHOFqT2fK"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gfXFOpAmR_dh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title 5.Download Result.mp4 to your computer\n",
        "from google.colab import files\n",
        "files.download('/content/Wav2Lip/results/result_voice.mp4') \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJtKN6ANUADM"
      },
      "source": [
        "# GFPGAN Inference Demo \n",
        "### (No colorization; No CUDA extensions required)\n",
        "\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2101.04061)\n",
        "[![GitHub Stars](https://img.shields.io/github/stars/TencentARC/GFPGAN?style=social)](https://github.com/TencentARC/GFPGAN)\n",
        "[![download](https://img.shields.io/github/downloads/TencentARC/GFPGAN/total.svg)](https://github.com/TencentARC/GFPGAN/releases)\n",
        "\n",
        "## GFPGAN - Towards Real-World Blind Face Restoration with Generative Facial Prior\n",
        "\n",
        "GFPGAN is a blind face restoration algorithm towards real-world face images. <br>\n",
        "It leverages the generative face prior in a pre-trained GAN (*e.g.*, StyleGAN2) to restore realistic faces while precerving fidelity. <br>\n",
        "\n",
        "If you want to use the paper model, please go to this [Colab Demo](https://colab.research.google.com/drive/1Oa1WwKB4M4l1GmR7CtswDVgOCOeSLChA?usp=sharing) for GFPGAN <a href=\"https://colab.research.google.com/drive/1Oa1WwKB4M4l1GmR7CtswDVgOCOeSLChA?usp=sharing\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"google colab logo\"></a>.\n",
        "\n",
        "**Limitations**: GFPGAN could not handle all the low-quality faces in the real world. Therefore, it may fail on your own cases.\n",
        "\n",
        "###Enjoy! :-)\n",
        "\n",
        "<img src=\"https://xinntao.github.io/projects/GFPGAN_src/gfpgan_teaser.jpg\" width=\"800\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY1Zbo3uUkXg"
      },
      "source": [
        "# 1. Preparations\n",
        "Before start, make sure that you choose\n",
        "* Runtime Type = Python 3\n",
        "* Hardware Accelerator = GPU\n",
        "\n",
        "in the **Runtime** menu -> **Change runtime type**\n",
        "\n",
        "Then, we clone the repository, set up the envrironment, and download the pre-trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwH2ifWEYEfJ",
        "outputId": "57991db5-5d29-4b33-9bcf-df5ca4d1775b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Clone GFPGAN and enter the GFPGAN folder\n",
        "%cd /content\n",
        "!rm -rf GFPGAN\n",
        "!git clone https://github.com/TencentARC/GFPGAN.git\n",
        "%cd GFPGAN\n",
        "\n",
        "# Set up the environment\n",
        "# Install basicsr - https://github.com/xinntao/BasicSR\n",
        "# We use BasicSR for both training and inference\n",
        "!pip install basicsr\n",
        "# Install facexlib - https://github.com/xinntao/facexlib\n",
        "# We use face detection and face restoration helper in the facexlib package\n",
        "!pip install facexlib\n",
        "# Install other depencencies\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "!pip install realesrgan  # used for enhancing the background (non-face) regions\n",
        "# Download the pre-trained model\n",
        "# !wget https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth -P experiments/pretrained_models\n",
        "# Now we use the V1.3 model for the demo\n",
        "!wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-8JxpPwg4Xz"
      },
      "source": [
        "# 2. Upload Images / Use the demo images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "CdMZYp0T7NAy",
        "outputId": "32646cd9-9cbe-455b-dbca-03ef364434c3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# upload your own images\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "upload_folder = 'inputs/upload'\n",
        "\n",
        "if os.path.isdir(upload_folder):\n",
        "    shutil.rmtree(upload_folder)\n",
        "os.mkdir(upload_folder)\n",
        "\n",
        "# upload images\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "  dst_path = os.path.join(upload_folder, filename)\n",
        "  print(f'move {filename} to {dst_path}')\n",
        "  shutil.move(filename, dst_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dZUYDlMeXKe"
      },
      "source": [
        "### OR you can use the demo image by running the following codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d6CkQ2x-eSjH",
        "outputId": "2d2e7ad8-f866-4d2a-8d49-53957a9eee70",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "upload_folder = 'inputs/upload'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C7EmHIzgSCb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Generate frames from output video\n",
        "import cv2\n",
        "# vid2frames('/content/result_voice.mp4','/content/GFPGAN/inputs/upload')\n",
        "import cv2\n",
        "\n",
        "# Opens the Video file\n",
        "cap= cv2.VideoCapture('/content/result_voice.mp4')\n",
        "i=0\n",
        "while(cap.isOpened()):\n",
        "    ret, frame = cap.read()\n",
        "    if ret == False:\n",
        "        break\n",
        "    cv2.imwrite('/content/GFPGAN/inputs/upload/image-'+str(i)+'.jpg',frame)\n",
        "    i+=1\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGHc73Up70ZA"
      },
      "source": [
        "# Generate enhanced frames by GFPGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmQVC3s97z4z",
        "outputId": "cbb189e0-4c84-475c-887b-176977da7658",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Now we use the GFPGAN to restore the above low-quality images\n",
        "# We use [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN) for enhancing the background (non-face) regions\n",
        "# You can find the different models in https://github.com/TencentARC/GFPGAN#european_castle-model-zoo\n",
        "!rm -rf results\n",
        "!python inference_gfpgan.py -i inputs/upload -o results -v 1.3 -s 2 --bg_upsampler realesrgan\n",
        "\n",
        "# Usage: python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2 [options]...\n",
        "# \n",
        "#  -h                   show this help\n",
        "#  -i input             Input image or folder. Default: inputs/whole_imgs\n",
        "#  -o output            Output folder. Default: results\n",
        "#  -v version           GFPGAN model version. Option: 1 | 1.2 | 1.3. Default: 1.3\n",
        "#  -s upscale           The final upsampling scale of the image. Default: 2\n",
        "#  -bg_upsampler        background upsampler. Default: realesrgan\n",
        "#  -bg_tile             Tile size for background sampler, 0 for no tile during testing. Default: 400\n",
        "#  -suffix              Suffix of the restored faces\n",
        "#  -only_center_face    Only restore the center face\n",
        "#  -aligned             Input are aligned faces\n",
        "#  -ext                 Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto\n",
        "\n",
        "!ls results/cmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkF8VAiF7-PY"
      },
      "source": [
        "# Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "tIeL_NJO8A3B",
        "outputId": "771a7da8-ed8a-4afd-c802-91d1bd8d40d1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# We first visualize the cropped faces\n",
        "# The left are the inputs images; the right are the results of GFPGAN\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "def display(img1, img2):\n",
        "  fig = plt.figure(figsize=(25, 10))\n",
        "  ax1 = fig.add_subplot(1, 2, 1) \n",
        "  plt.title('Input image', fontsize=16)\n",
        "  ax1.axis('off')\n",
        "  ax2 = fig.add_subplot(1, 2, 2)\n",
        "  plt.title('GFPGAN output', fontsize=16)\n",
        "  ax2.axis('off')\n",
        "  ax1.imshow(img1)\n",
        "  ax2.imshow(img2)\n",
        "def imread(img_path):\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  return img\n",
        "\n",
        "# display each image in the upload folder\n",
        "import os\n",
        "import glob\n",
        "\n",
        "input_folder = 'results/cropped_faces'\n",
        "result_folder = 'results/restored_faces'\n",
        "input_list = sorted(glob.glob(os.path.join(input_folder, '*')))\n",
        "output_list = sorted(glob.glob(os.path.join(result_folder, '*')))\n",
        "for input_path, output_path in zip(input_list, output_list):\n",
        "  img_input = imread(input_path)\n",
        "  img_output = imread(output_path)\n",
        "  display(img_input, img_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Jn_2ylqP9qXY",
        "outputId": "5dec0566-acea-426a-a7b6-6f5dc19eec89",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# We then visualize the whole image\n",
        "# The left are the inputs images; the right are the results of GFPGAN\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "def display(img1, img2):\n",
        "  fig = plt.figure(figsize=(25, 10))\n",
        "  ax1 = fig.add_subplot(1, 2, 1) \n",
        "  plt.title('Input image', fontsize=16)\n",
        "  ax1.axis('off')\n",
        "  ax2 = fig.add_subplot(1, 2, 2)\n",
        "  plt.title('GFPGAN output', fontsize=16)\n",
        "  ax2.axis('off')\n",
        "  ax1.imshow(img1)\n",
        "  ax2.imshow(img2)\n",
        "def imread(img_path):\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  return img\n",
        "\n",
        "# display each image in the upload folder\n",
        "import os\n",
        "import glob\n",
        "\n",
        "input_folder = 'inputs/upload'\n",
        "result_folder = 'results/restored_imgs'\n",
        "input_list = sorted(glob.glob(os.path.join(input_folder, '*')))\n",
        "output_list = sorted(glob.glob(os.path.join(result_folder, '*')))\n",
        "for input_path, output_path in zip(input_list, output_list):\n",
        "  img_input = imread(input_path)\n",
        "  img_output = imread(output_path)\n",
        "  display(img_input, img_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Make Video from Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pep6JDh5utUX",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "\n",
        "files = os.listdir('/content/GFPGAN/results/restored_imgs/')\n",
        "filenames = files.sort(key=lambda sort_files: list(map(int, re.findall(r'\\d+', sort_files)))[0])\n",
        "img_array = []\n",
        "\n",
        "for filename in filenames:\n",
        "    \n",
        "    # if filename.split('.')[-1]=='jpg':\n",
        "    img = cv2.imread('/content/GFPGAN/results/restored_imgs/'+filename)\n",
        "    height, width, layers = img.shape\n",
        "    size = (width,height)\n",
        "    img_array.append(img)\n",
        "\n",
        "\n",
        "out = cv2.VideoWriter('output.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 25, size)\n",
        " \n",
        "for i in range(len(img_array)):\n",
        "    out.write(img_array[i])\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add audio to the video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaPAnTRwBSUr",
        "outputId": "c5bc89d1-c0db-4473-8bac-a6e92be133f2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!ffmpeg -i /content/GFPGAN/proj.mp4 -i /content/result_voice.mp4 -map 0:v -map 1:a -c:v copy -shortest tamim.mp4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "gcKQLjzbw6Lg",
        "outputId": "21c67a92-41ae-4188-f99c-de3fa0b18864",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title 4.Play result video -  50% scaling\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('/content/GFPGAN/tamim.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"50%\" height=\"50%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR7VEBEb8slX"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## 5. Download results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "zuBCgeH08tdn",
        "outputId": "59c7c9d8-b601-4378-99ae-079d5b7ab85b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# download the result\n",
        "!ls results\n",
        "print('Download results')\n",
        "os.system('zip -r download.zip results')\n",
        "files.download(\"download.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "sE22UQtQ9YYi",
        "yJ5taGmPcWV-"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
